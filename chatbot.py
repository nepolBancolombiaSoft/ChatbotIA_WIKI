import os
import cohere
import langdetect
import mtranslate
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from config import COHERE_API_KEY  # âœ… AsegÃºrate de tener tu clave en config.py

# ğŸ“Œ Configurar API de Cohere
co = cohere.Client(COHERE_API_KEY)

# ğŸ“Œ ConfiguraciÃ³n de la Base de Datos
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(BASE_DIR, "wiki_db")

db = Chroma(
    persist_directory=DB_PATH,
    embedding_function=HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-small")
)
retriever = db.as_retriever()
print(f"ğŸ“‚ Base de datos guardada en: {DB_PATH}")

# ğŸ“Œ Historial de conversaciÃ³n
historial = []

# ğŸ“Œ Prompt optimizado para Cohere
qa_prompt = PromptTemplate(
    input_variables=["context", "question", "history"],
    template=(
        "Responde de manera clara y precisa usando la informaciÃ³n de la Wiki.\n\n"
        "**Historial de conversaciÃ³n:**\n{history}\n\n"
        "**InformaciÃ³n relevante:**\n{context}\n\n"
        "**Pregunta:** {question}\n\n"
        "ğŸ“Œ **ExplicaciÃ³n clara y estructurada:**"
    )
)

# ğŸ“Œ FunciÃ³n para actualizar el historial de conversaciÃ³n
def actualizar_historial(pregunta, respuesta):
    historial.append(f"Usuario: {pregunta}\nChatbot: {respuesta}")
    if len(historial) > 5:  # Mantiene las Ãºltimas 5 interacciones
        historial.pop(0)

# ğŸ“Œ FunciÃ³n para generar respuesta con Cohere API
def generar_respuesta(pregunta):
    # ğŸ“Œ Buscar documentos relevantes
    documentos = retriever.invoke(pregunta)
    print(f"ğŸ” Debug: {len(documentos)} documentos encontrados para '{pregunta}'")

    documentos_relevantes = []
    
    if documentos:
        print(f"\nğŸ“„ {len(documentos)} documentos relevantes encontrados.")

        # ğŸ“Œ Evitar documentos duplicados
        documentos_unicos = {}
        for idx, doc in enumerate(documentos, 1):
            url = doc.metadata.get("source", "âš ï¸ No disponible")
            contenido = doc.page_content[:400]  # Muestra los primeros 400 caracteres del contenido
            
            documentos_relevantes.append({"url": url, "contenido": contenido})

            # Guardar solo documentos Ãºnicos basados en la URL
            if url not in documentos_unicos:
                documentos_unicos[url] = contenido

        print("âœ… Los documentos fueron encontrados e impresos correctamente.")

        # ğŸ“Œ Generar respuesta con Cohere API
        print("\nğŸ¤” Estoy pensando en la respuesta...")  # âœ… Mensaje mientras se genera la IA

        query = qa_prompt.format(
            question=pregunta,
            context="\n\n".join(documentos_unicos.values()),
            history="\n".join(historial)
        )

        respuesta = co.generate(
            model="command",
            prompt=query,
            max_tokens=800,  # âœ… Permite respuestas mÃ¡s largas
            temperature=0.3,  # âœ… Hace que la respuesta sea mÃ¡s precisa y menos aleatoria
            frequency_penalty=0.4,  # âœ… Reduce repeticiones
            presence_penalty=0.5,  # âœ… Asegura que la IA no invente informaciÃ³n
            stop_sequences=["\n"]
        )

        respuesta_texto = respuesta.generations[0].text.strip()

        # ğŸ“Œ Si la respuesta estÃ¡ en inglÃ©s, traducir automÃ¡ticamente
        if langdetect.detect(respuesta_texto) == "en":
            print("ğŸŒ DetectÃ© que la respuesta estÃ¡ en inglÃ©s. Traduciendo al espaÃ±ol...")
            respuesta_texto = mtranslate.translate(respuesta_texto, "es", "auto")

        # ğŸ“Œ Guardar en el historial
        actualizar_historial(pregunta, respuesta_texto)

        return respuesta_texto, documentos_relevantes

    else:
        return "âš ï¸ No encontrÃ© informaciÃ³n relevante.", []

